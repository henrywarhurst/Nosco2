package com.henrywarhurst.facerecog;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;

import org.opencv.android.BaseLoaderCallback;
import org.opencv.android.CameraBridgeViewBase.CvCameraViewFrame;
import org.opencv.android.LoaderCallbackInterface;
import org.opencv.android.OpenCVLoader;
import org.opencv.core.CvType;
import org.opencv.core.Mat;
import org.opencv.core.MatOfRect;
import org.opencv.core.Point;
import org.opencv.core.Rect;
import org.opencv.core.Scalar;
import org.opencv.core.Size;
import org.opencv.imgproc.Imgproc;
import org.opencv.core.Core;
import org.opencv.android.CameraBridgeViewBase;
import org.opencv.android.CameraBridgeViewBase.CvCameraViewListener2;
import org.opencv.objdetect.CascadeClassifier;

import android.app.Activity;
import android.content.Context;
import android.content.Intent;
import android.net.Uri;
import android.os.Bundle;
import android.os.Environment;
import android.speech.tts.TextToSpeech;
import android.util.Log;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.view.WindowManager;
import android.widget.ImageView;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Locale;

public class FdActivity extends Activity implements CvCameraViewListener2,
		TextToSpeech.OnInitListener {

	private static final String TAG = "OCVSample::Activity";
	private static final Scalar FACE_RECT_COLOR = new Scalar(0, 255, 0, 255);
	public static final int JAVA_DETECTOR = 0;
	public static final int NATIVE_DETECTOR = 1;

	private MenuItem mItemFace50;
	private MenuItem mItemFace40;
	private MenuItem mItemFace30;
	private MenuItem mItemFace20;
	private MenuItem mItemType;

	private Mat mRgba;
	private Mat mGray;
	private File mCascadeFile;
	private CascadeClassifier mJavaDetector;
	private DetectionBasedTracker mNativeDetector;

	private int mDetectorType = NATIVE_DETECTOR;
	private String[] mDetectorName;

	private float mRelativeFaceSize = 0.2f;
	private int mAbsoluteFaceSize = 0;

	private CameraBridgeViewBase mOpenCvCameraView;

	private TextToSpeech myTTS;

	private final String imgPath = Environment.DIRECTORY_PICTURES;

	// Counts how many times same person seen in a row.
	private int seenCount;
	// How many frames should we wait in between announcements?
	private int speakPeriod = 50;
	// List of who we think we've seen
	private ArrayList<Integer> observedIds;
	// Who should we speak the name of?
	private String speakName;
	// Should we speak at all?
	private boolean volumeMuted = false;
	// The smoothing Hidden Markov Model
	Hmm hmm;
	// The database
	private PeopleDataSource datasource;
	private List<Person> allPeople;

	private int MY_DATA_CHECK_CODE;

	private FaceRec2 faceRecognizer;

	private BaseLoaderCallback mLoaderCallback = new BaseLoaderCallback(this) {
		@Override
		public void onManagerConnected(int status) {
			switch (status) {
			case LoaderCallbackInterface.SUCCESS: {
				Log.i(TAG, "OpenCV loaded successfully");

				// Load native library after(!) OpenCV initialization
				System.loadLibrary("detection_based_tracker");
				System.loadLibrary("adaptive_histogram");
				System.loadLibrary("face_recog_native");
				System.loadLibrary("hmm_native");

				// Model setup
				faceRecognizer = new FaceRec2();
				faceRecognizer.train();
				hmm = new Hmm(faceRecognizer.numSubjects());
				ArrayList<Integer> samp = new ArrayList<Integer>(Arrays.asList(48, 48, 48, 48, 48, 48, 48, 48));
				Log.e(TAG, Integer.toString(faceRecognizer.numSubjects()));

				try {
					// load cascade file from application resources
					InputStream is = getResources().openRawResource(
							R.raw.lbpcascade_frontalface);
					File cascadeDir = getDir("cascade", Context.MODE_PRIVATE);
					mCascadeFile = new File(cascadeDir,
							"lbpcascade_frontalface.xml");
					FileOutputStream os = new FileOutputStream(mCascadeFile);

					byte[] buffer = new byte[4096];
					int bytesRead;
					while ((bytesRead = is.read(buffer)) != -1) {
						os.write(buffer, 0, bytesRead);
					}
					is.close();
					os.close();

					mJavaDetector = new CascadeClassifier(
							mCascadeFile.getAbsolutePath());
					if (mJavaDetector.empty()) {
						Log.e(TAG, "Failed to load cascade classifier");
						mJavaDetector = null;
					} else
						Log.i(TAG, "Loaded cascade classifier from "
								+ mCascadeFile.getAbsolutePath());

					mNativeDetector = new DetectionBasedTracker(
							mCascadeFile.getAbsolutePath(), 0);

					cascadeDir.delete();

				} catch (IOException e) {
					e.printStackTrace();
					Log.e(TAG, "Failed to load cascade. Exception thrown: " + e);
				}

				mOpenCvCameraView.enableView();
			}
				break;
			default: {
				super.onManagerConnected(status);
			}
				break;
			}
		}
	};

	public FdActivity() {
		mDetectorName = new String[2];
		mDetectorName[JAVA_DETECTOR] = "Java";
		mDetectorName[NATIVE_DETECTOR] = "Native (tracking)";

		Log.i(TAG, "Instantiated new " + this.getClass());
	}

	/** Called when the activity is first created. */
	@Override
	public void onCreate(Bundle savedInstanceState) {
		Log.i(TAG, "called onCreate");
		super.onCreate(savedInstanceState);
		getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);
		setContentView(R.layout.activity_fd);

		// TTS setup
		myTTS = new TextToSpeech(this, this);
		MY_DATA_CHECK_CODE = 0;
		Intent checkTTSIntent = new Intent();
		checkTTSIntent.setAction(TextToSpeech.Engine.ACTION_CHECK_TTS_DATA);
		startActivityForResult(checkTTSIntent, MY_DATA_CHECK_CODE);

		// Interface with the database
		datasource = new PeopleDataSource(this);
		datasource.open();
		allPeople = datasource.getAllPeople();

		seenCount = 0;
		speakName = "";
		observedIds = new ArrayList<Integer>();

		mOpenCvCameraView = (CameraBridgeViewBase) findViewById(R.id.fd_activity_surface_view);
		mOpenCvCameraView.setCvCameraViewListener(this);
	}

	@Override
	public void onPause() {
		super.onPause();
		if (mOpenCvCameraView != null)
			mOpenCvCameraView.disableView();
	}

	@Override
	public void onResume() {
		super.onResume();
		if (!OpenCVLoader.initDebug()) {
			Log.d(TAG,
					"Internal OpenCV library not found. Using OpenCV Manager for initialization");
			OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_3_0_0, this,
					mLoaderCallback);
		} else {
			Log.d(TAG, "OpenCV library found inside package. Using it!");
			mLoaderCallback.onManagerConnected(LoaderCallbackInterface.SUCCESS);
		}
	}

	public void onDestroy() {
		super.onDestroy();
		mOpenCvCameraView.disableView();
		// Memory deallocation
		faceRecognizer.release();
		mNativeDetector.release();
		hmm.release();
	}

	public void onCameraViewStarted(int width, int height) {
		mGray = new Mat();
		mRgba = new Mat();
	}

	public void onCameraViewStopped() {
		mGray.release();
		mRgba.release();
	}

	public Mat onCameraFrame(CvCameraViewFrame inputFrame) {
		mRgba = inputFrame.rgba();
		mGray = inputFrame.gray();

		if (mAbsoluteFaceSize == 0) {
			int height = mGray.rows();
			if (Math.round(height * mRelativeFaceSize) > 0) {
				mAbsoluteFaceSize = Math.round(height * mRelativeFaceSize);
			}
			mNativeDetector.setMinFaceSize(mAbsoluteFaceSize);
		}

		MatOfRect faces = new MatOfRect();

		if (mDetectorType == JAVA_DETECTOR) {
			if (mJavaDetector != null)
				mJavaDetector.detectMultiScale(mGray, faces, 1.1, 2,
						2, // TODO: objdetect.CV_HAAR_SCALE_IMAGE
						new Size(mAbsoluteFaceSize, mAbsoluteFaceSize),
						new Size());
		} else if (mDetectorType == NATIVE_DETECTOR) {
			if (mNativeDetector != null)
				mNativeDetector.detect(mGray, faces);
		} else {
			Log.e(TAG, "Detection method is not selected!");
		}

		Rect[] facesArray = faces.toArray();
		for (int i = 0; i < facesArray.length; i++) {
			if (Utility.roiSizeOk(mRgba, facesArray[i]))
				// Draws the rectangle tl = top left, br = bottom right
				Imgproc.rectangle(mRgba, facesArray[i].tl(),
						facesArray[i].br(), FACE_RECT_COLOR, 3);
		}

		if (facesArray.length != 0 && Utility.roiSizeOk(mGray, facesArray[0])) {
			// Resize image to 100x100
			Mat resizedImg = new Mat();
			Size size = new Size(100, 100);
			Imgproc.resize(mGray.submat(facesArray[0]), resizedImg, size);
			Mat claheImg = new Mat();
			// AdaptiveHistogram.doClahe(
			// resizedImg.getNativeObjAddr(), claheImg.getNativeObjAddr());
			Imgproc.equalizeHist(resizedImg, claheImg);
			Prediction pred = faceRecognizer.predict(claheImg);
			long curId = pred.getPersonId();
			observedIds.add((int) curId);
			String[] name = findPerson((int) curId);			
			if (name[0] != null) {
				Imgproc.putText(mRgba, name[0],
						new Point(facesArray[0].x, facesArray[0].y), 3, 1,
						new Scalar(255, 0, 0, 255), 1);
			}
			
			if (observedIds.size() >= speakPeriod) {
				int periodId = hmm.get_likely_subject(observedIds);
				name = findPerson(periodId);
				speakText("Recognised " + name[0]);
				// Clean slate
				observedIds.clear();
			}
		}
		return mRgba;
	}

	@Override
	public boolean onCreateOptionsMenu(Menu menu) {
		Log.i(TAG, "called onCreateOptionsMenu");
		mItemFace50 = menu.add("Face size 50%");
		mItemFace40 = menu.add("Face size 40%");
		mItemFace30 = menu.add("Face size 30%");
		mItemFace20 = menu.add("Face size 20%");
		mItemType = menu.add(mDetectorName[mDetectorType]);
		return true;
	}

	@Override
	public boolean onOptionsItemSelected(MenuItem item) {
		Log.i(TAG, "called onOptionsItemSelected; selected item: " + item);
		if (item == mItemFace50)
			setMinFaceSize(0.5f);
		else if (item == mItemFace40)
			setMinFaceSize(0.4f);
		else if (item == mItemFace30)
			setMinFaceSize(0.3f);
		else if (item == mItemFace20)
			setMinFaceSize(0.2f);
		else if (item == mItemType) {
			int tmpDetectorType = (mDetectorType + 1) % mDetectorName.length;
			item.setTitle(mDetectorName[tmpDetectorType]);
			setDetectorType(tmpDetectorType);
		}
		return true;
	}

	private void setMinFaceSize(float faceSize) {
		mRelativeFaceSize = faceSize;
		mAbsoluteFaceSize = 0;
	}

	private void setDetectorType(int type) {
		if (mDetectorType != type) {
			mDetectorType = type;

			if (type == NATIVE_DETECTOR) {
				Log.i(TAG, "Detection Based Tracker enabled");
				mNativeDetector.start();
			} else {
				Log.i(TAG, "Cascade detector enabled");
				mNativeDetector.stop();
			}
		}
	}

	private void speakText(String text) {
		if (!volumeMuted)
			myTTS.speak(text, TextToSpeech.QUEUE_ADD, null);
	}

	public void trainFaces(View view) {
		Intent intent = new Intent(this, FacesLibrary.class);
		startActivity(intent);
	}

	public void muteUnmute(View view) {
		volumeMuted = !volumeMuted;
		ImageView img = (ImageView) findViewById(R.id.button_mute);
		if (volumeMuted) {
			img.setImageResource(R.drawable.ic_volume_mute_white_24dp);
		} else {
			img.setImageResource(R.drawable.ic_volume_up_white_24dp);
		}
	}

	// setup TTS
	public void onInit(int initStatus) {
		// check for successful instantiation
		if (initStatus == TextToSpeech.SUCCESS) {
			if (myTTS.isLanguageAvailable(Locale.ENGLISH) == TextToSpeech.LANG_AVAILABLE)
				myTTS.setLanguage(Locale.ENGLISH);
		} else if (initStatus == TextToSpeech.ERROR) {
			Log.e(TAG, "TTS onInit Error!");
		}
	}

	public String[] findPerson(int curId) {
		String[] name = new String[2];
		for (Person p : allPeople) {
			if ((int) p.getId() == curId) {
				name[0] = p.getFirstname();
				name[1] = p.getLastname();
			}
		}
		return name;
	}
}
